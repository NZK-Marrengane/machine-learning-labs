{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2cc3ec",
   "metadata": {},
   "source": [
    "> ## Key Takeaways\n",
    "\n",
    "> By the end of this module, students will gain a comprehensive understanding of sentiment analysis techniques, be able to preprocess text data effectively, build and evaluate sentiment analysis models using both traditional machine learning algorithms and deep learning techniques, and apply these skills to real-world NLP tasks.\n",
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f25262",
   "metadata": {},
   "source": [
    "### 1. Introduction to NLP\n",
    "- Understanding Natural Language Processing (NLP) and its applications.\n",
    "- Overview of NLP techniques for text analysis and understanding.\n",
    "- Importance of sentiment analysis in understanding customer opinions and feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137b225",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing\n",
    "- __Tokenization:__ Breaking down text into smaller units such as words or sentences.\n",
    "- __Stopword Removal:__ Removing common words (e.g., \"the\", \"is\", \"and\") that do not carry significant meaning.\n",
    "- __Lemmatization:__ Reducing words to their base or root form (e.g., \"running\" to \"run\", \"better\" to \"good\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec901af",
   "metadata": {},
   "source": [
    "### 3. Bag of Words Model\n",
    "- Introduction to the Bag of Words (BoW) model.\n",
    "- Creating a vocabulary of unique words from the text corpus.\n",
    "- Representing text documents as numerical vectors based on word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722661ec",
   "metadata": {},
   "source": [
    "### 4. Sentiment Analysis Algorithms\n",
    "\n",
    "#### Naive Bayes Classifier:\n",
    "\n",
    "- Understanding the Naive Bayes algorithm for text classification.\n",
    "- Training a Naive Bayes classifier using labeled data for sentiment analysis.\n",
    "- Evaluating the classifier performance using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "#### LSTM (Long Short-Term Memory) Model:\n",
    "\n",
    "- Introduction to Recurrent Neural Networks (RNNs) and LSTM architecture.\n",
    "- Preprocessing text data for LSTM input.\n",
    "- Building and training an LSTM model for sentiment analysis.\n",
    "- Fine-tuning the model and optimizing hyperparameters.\n",
    "- Evaluating the LSTM model performance and comparing it with traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37b139",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation Metrics\n",
    "\n",
    "- __Accuracy:__ The proportion of correctly classified instances.\n",
    "- __Precision:__ The proportion of true positive predictions among all positive predictions.\n",
    "- __Recall:__ The proportion of true positive predictions among all actual positive instances.\n",
    "- __F1-score:__ The harmonic mean of precision and recall, providing a balanced measure of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f135332",
   "metadata": {},
   "source": [
    "### 6. Practical Applications and Case Studies\n",
    "- Sentiment analysis on customer reviews for products or services.\n",
    "- Analyzing social media sentiments for brand perception.\n",
    "- Understanding public opinions on political or social issues through text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f672ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=Warning, lineno=0, append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed75f3",
   "metadata": {},
   "source": [
    "## **Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bf18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"Wow... love this place\", \n",
    "    \"Crust is not good\", \n",
    "    \"Not tasty and the texture was just nasty\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f4db85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kennedy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lower = [text.lower() for text in text_data]\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0519f96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', '...', 'love', 'this', 'place'],\n",
       " ['crust', 'is', 'not', 'good'],\n",
       " ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_data = [nltk.word_tokenize(text) for text in text_lower]\n",
    "token_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "795adf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "english_stopwatch = set(stopwords.words('english'))\n",
    "english_stopwatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb4b396c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crust is not good'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Crust is not good'.translate(str.maketrans('', '', string.punctuation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
